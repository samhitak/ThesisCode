{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime as dt\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "import math\n",
    "import sklearn as sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from random import randint\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv('users.csv', encoding = \"ISO-8859-1\")\n",
    "articles = pd.read_csv('articles.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take interactions that have sharerHashes\n",
    "forgraph_t = articles[articles['sharerHash'] != '[]']\n",
    "forgraph_t = forgraph_t.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take interactions that have userHashes that are in the users table\n",
    "users_list = set(users[\"userHash\"])\n",
    "forgraph = forgraph_t[forgraph_t[\"userHash\"].isin(users_list)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomInteraction(all_interactions):\n",
    "    index = randint(0, len(all_interactions)-1)\n",
    "    currUserHash = all_interactions.iloc[[index]]['userHash'][index]\n",
    "    currSharerHash_list = ast.literal_eval(all_interactions.iloc[[index]]['sharerHash'][index])\n",
    "    currSeenTime_list = ast.literal_eval(all_interactions.iloc[[index]]['articleSeenTime'][index])\n",
    "    if len(currSharerHash_list) == 1:\n",
    "        currSharerHash = currSharerHash_list[0]\n",
    "        currSeenTime = currSeenTime_list[0]\n",
    "    else:\n",
    "        t_index = randint(0, len(currSharerHash_list)-1)\n",
    "        currSharerHash = currSharerHash_list[t_index]\n",
    "        currSeenTime= currSeenTime_list[t_index]\n",
    "    return [str(currUserHash), str(currSharerHash), str(currSeenTime)]\n",
    "\n",
    "def makeGraph(timestamp, graph_data):\n",
    "    timestamp = dt.strptime(timestamp, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    beforetime_DF = pd.DataFrame()\n",
    "    for i in range(0, len(graph_data)):\n",
    "        temp_date_list = ast.literal_eval(graph_data.iloc[[i]][\"articleSeenTime\"][i])\n",
    "        for k in range(0, len(temp_date_list)):\n",
    "            temp_date = temp_date_list[k]\n",
    "            date_obj = dt.strptime(temp_date, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "            if date_obj <= timestamp:\n",
    "                beforetime_DF = beforetime_DF.append(graph_data.iloc[[i]], ignore_index=True)\n",
    "                break\n",
    "                \n",
    "    sharerHash_list = beforetime_DF['sharerHash']\n",
    "    userHash_list = beforetime_DF[\"userHash\"]\n",
    "    \n",
    "    edges_list = []\n",
    "    for i in range(0, len(sharerHash_list)):\n",
    "        temp = ast.literal_eval(sharerHash_list[i])\n",
    "        for k in range(0, len(temp)):\n",
    "            edges_list.append((str(userHash_list[i]), temp[k]))\n",
    "            \n",
    "    weighted_edges = []\n",
    "    i = 0\n",
    "    while i < len(edges_list)-2:\n",
    "        weight = 1\n",
    "        while ((edges_list[i][0] == edges_list[i+1][0]) and (edges_list[i][1] == edges_list[i+1][1])):\n",
    "            weight = weight + 1\n",
    "            i = i + 1\n",
    "        weighted_edges.append((edges_list[i][0], edges_list[i][1], weight))\n",
    "        i = i + 1\n",
    "        \n",
    "    interactions = nx.DiGraph()\n",
    "    interactions.add_weighted_edges_from(weighted_edges)\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thematicInterest(rand_userHash, users, rand_user_article):\n",
    "    # get article data\n",
    "    article_data = getArticleData(rand_user_article)\n",
    "    #article_counts = Counter(stem(removeStopwords(tokenize(article_data))))\n",
    "    #print (article_data)\n",
    "    \n",
    "    u_data = getUserData(rand_userHash, users)\n",
    "    #user_counts = Counter(stem(removeStopwords(tokenize(u_data))))\n",
    "    #print (u_data)\n",
    "    \n",
    "    docs = [article_data, u_data]\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    output = tfidf.fit_transform(docs)\n",
    "    cosine_similarities = linear_kernel(output[0:1], output).flatten()\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    output = tfidf.fit_transform(docs)\n",
    "    cosine_similarities = linear_kernel(output[0:1], output).flatten()\n",
    "    return cosine_similarities[1]\n",
    "\n",
    "def activity(x, article_data):\n",
    "    this_user = article_data[article_data[\"userHash\"] == int(x)]\n",
    "    indeg = len(this_user.index) + 0.01\n",
    "    shared_by_user = article_data[article_data[\"sharerHash\"] == int(x)]\n",
    "    outdeg = len(shared_by_user.index) + 0.01\n",
    "    return outdeg/indeg\n",
    "\n",
    "def socialPressure(x, article_data, rand_user_article):\n",
    "    sharers = list(set(article_data[article_data[\"userHash\"] == int(x)][\"sharerHash\"]))\n",
    "    R = len(sharers)\n",
    "    counter = 0\n",
    "    for index,row in article_data.iterrows():\n",
    "        if row[\"sharerHash\"] in sharers and row[\"articleURL\"] == rand_user_article[\"articleURL\"]:\n",
    "            counter = counter + 1\n",
    "    return counter/R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getArticleData(rand_article):\n",
    "    comments = ast.literal_eval(rand_article[\"articleComments\"])[0]\n",
    "    title = rand_article[\"articleTitle\"]\n",
    "    if (comments == \"NULL\"):\n",
    "        comments = \"\"\n",
    "    if (title == \"NULL\"):\n",
    "        title = \"\"\n",
    "    article_data = comments + \" \" + title\n",
    "    return article_data\n",
    "\n",
    "def getUserData(userHash, users): #user_info is a df that is a single row\n",
    "    user_info = users[users[\"userHash\"] == userHash].reset_index()\n",
    "    u_data = \"\"\n",
    "    if str(user_info[\"currentLocation\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"currentLocation\"][0]) + \" \"\n",
    "    if str(user_info[\"education1location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"education1location\"][0]) + \" \"\n",
    "    if str(user_info[\"education1name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"education1name\"][0]) + \" \"\n",
    "    if str(user_info[\"education2location\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education2location\"][0]) + \" \"\n",
    "    if str(user_info[\"education2name\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education2name\"][0]) + \" \"\n",
    "    if str(user_info[\"education3location\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education3location\"][0]) + \" \"\n",
    "    if str(user_info[\"education3name\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education3name\"][0]) + \" \"\n",
    "    if str(user_info[\"gender\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"gender\"][0]) + \" \"\n",
    "    if str(user_info[\"hometown\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"hometown\"][0]) + \" \"\n",
    "    if str(user_info[\"language1\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language1\"][0]) + \" \"\n",
    "    if str(user_info[\"language2\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language2\"][0]) + \" \"\n",
    "    if str(user_info[\"language3\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language3\"][0]) + \" \"\n",
    "    if str(user_info[\"otherLocation1\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"otherLocation1\"][0]) + \" \"\n",
    "    if str(user_info[\"otherLocation2\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"otherLocation2\"][0]) + \" \"\n",
    "    if str(user_info[\"work1location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1location\"][0]) + \" \"\n",
    "    if str(user_info[\"work1name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1name\"][0]) + \" \"\n",
    "    if str(user_info[\"work1role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1role\"][0]) + \" \"\n",
    "    if str(user_info[\"work2location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2location\"][0]) + \" \"\n",
    "    if str(user_info[\"work2name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2name\"][0]) + \" \"\n",
    "    if str(user_info[\"work2role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2role\"][0]) + \" \"\n",
    "    if str(user_info[\"work3location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3location\"][0]) + \" \"\n",
    "    if str(user_info[\"work3name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3name\"][0]) + \" \"\n",
    "    if str(user_info[\"work3role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3role\"][0]) + \" \"\n",
    "    if str(user_info[\"work4location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4location\"][0]) + \" \"\n",
    "    if str(user_info[\"work4name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4name\"][0]) + \" \"\n",
    "    if str(user_info[\"work4role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4role\"][0]) + \" \"\n",
    "    return u_data\n",
    "\n",
    "def getUserArticleSim(article_data, u_data):\n",
    "    docs = [article_data, u_data]\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    output = tfidf.fit_transform(docs)\n",
    "    cosine_similarities = linear_kernel(output[0:1], output).flatten()\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_userHashes = users[\"userHash\"]\n",
    "articles_userHashes = forgraph[\"userHash\"]\n",
    "valid_userHashes = list(set(users_userHashes) & set(articles_userHashes))\n",
    "\n",
    "valid_interactions = forgraph[forgraph[\"userHash\"].isin(valid_userHashes)]\n",
    "\n",
    "count = 0\n",
    "while len(features) < 1000:\n",
    "    print(count)\n",
    "    random_userHash = random.choice(valid_userHashes)\n",
    "\n",
    "    user_articles = valid_interactions[valid_interactions['userHash'] == random_userHash]\n",
    "    user_articles_list = user_articles[\"articleURL\"]\n",
    "    \n",
    "    random_valid_article = user_articles.sample(n=1).reset_index().iloc(0)\n",
    "    random_valid_article = random_valid_article[0]\n",
    "    \n",
    "    timestamp = ast.literal_eval(random_valid_article[\"articleSeenTime\"])[0]\n",
    "    timestamp = dt.strptime(timestamp, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    beforetime_DF = pd.DataFrame()\n",
    "    for index, row in valid_interactions.iterrows():\n",
    "        for i in ast.literal_eval(row[\"articleSeenTime\"]):\n",
    "            date_obj = dt.strptime(i, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "            if date_obj <= timestamp:\n",
    "                beforetime_DF = beforetime_DF.append(row, ignore_index=True)\n",
    "                break\n",
    "                \n",
    "    timestamp_n = getExpectedTime(random_userHash, valid_interactions)\n",
    "    beforetime_DF_n = pd.DataFrame()\n",
    "    for index, row in valid_interactions.iterrows():\n",
    "        for i in ast.literal_eval(row[\"articleSeenTime\"]):\n",
    "            date_obj = dt.strptime(i, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "            if date_obj <= timestamp_n:\n",
    "                beforetime_DF_n = beforetime_DF_n.append(row, ignore_index=True)\n",
    "                break\n",
    "    \n",
    "    non_user_articles = beforetime_DF[beforetime_DF['userHash'] != random_userHash]\n",
    "    random_notseen_article = non_user_articles.sample(n=1).reset_index().iloc(0)\n",
    "    random_notseen_article = random_notseen_article[0]\n",
    "    while random_notseen_article[\"articleURL\"] in user_articles_list:\n",
    "        random_notseen_article = non_user_articles.sample(n=1).reset_index().iloc(0)\n",
    "        random_notseen_article = random_notseen_article[0]\n",
    "    \n",
    "    s_t = thematicInterest(random_userHash, users, random_valid_article)\n",
    "    s_f = thematicInterest(random_userHash, users, random_notseen_article)\n",
    "    a_t = activity(str(random_userHash), beforetime_DF)\n",
    "    a_f = activity(str(random_userHash), beforetime_DF_n)\n",
    "    p_t = socialPressure(str(random_userHash), beforetime_DF, random_valid_article)\n",
    "    p_f = socialPressure(str(random_userHash), beforetime_DF_n, random_notseen_article)\n",
    "    \n",
    "\n",
    "    features.append([s_t, a_t, p_t, 1])\n",
    "    features.append([s_f, a_f, p_f, 0])\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.asarray(features)[:,:3]\n",
    "y = np.asarray(features)[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(fit_intercept=False)\n",
    "y_o, predicted_ridge_reg = ridge_reg.fit(X, y, cross_val=True)\n",
    "ridge_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_p, tpr_p, thresholds_p = roc_curve(y_o, predicted_ridge_reg)\n",
    "roc_auc_p = auc(fpr_p, tpr_p)\n",
    "print (\"Area under the PA ROC curve : %f\" % roc_auc_p)\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.plot(fpr_p, tpr_p, color='blue', label='Diffusion Model ROC curve (area = %0.2f)' % roc_auc_p)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Diffusion Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('diffusion_model_1000samples_lreg.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
