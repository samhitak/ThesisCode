{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime as dt\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "import math\n",
    "import sklearn as sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from random import randint\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv('users.csv', encoding = \"ISO-8859-1\")\n",
    "articles = pd.read_csv('articles.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take interactions that have sharerHashes\n",
    "forgraph_t = articles[articles['sharerHash'] != '[]']\n",
    "forgraph_t = forgraph_t.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take interactions that have userHashes that are in the users table\n",
    "users_list = set(users[\"userHash\"])\n",
    "forgraph = forgraph_t[forgraph_t[\"userHash\"].isin(users_list)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomInteraction(all_interactions):\n",
    "    index = randint(0, len(all_interactions)-1)\n",
    "    currUserHash = all_interactions.iloc[[index]]['userHash'][index]\n",
    "    currSharerHash_list = ast.literal_eval(all_interactions.iloc[[index]]['sharerHash'][index])\n",
    "    currSeenTime_list = ast.literal_eval(all_interactions.iloc[[index]]['articleSeenTime'][index])\n",
    "    if len(currSharerHash_list) == 1:\n",
    "        currSharerHash = currSharerHash_list[0]\n",
    "        currSeenTime = currSeenTime_list[0]\n",
    "    else:\n",
    "        t_index = randint(0, len(currSharerHash_list)-1)\n",
    "        currSharerHash = currSharerHash_list[t_index]\n",
    "        currSeenTime= currSeenTime_list[t_index]\n",
    "    return [str(currUserHash), str(currSharerHash), str(currSeenTime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thematicInterest(rand_userHash, users, rand_user_article):\n",
    "    # get article data\n",
    "    article_data = getArticleData(rand_user_article)\n",
    "    #article_counts = Counter(stem(removeStopwords(tokenize(article_data))))\n",
    "    #print (article_data)\n",
    "    \n",
    "    u_data = getUserData(rand_userHash, users)\n",
    "    #user_counts = Counter(stem(removeStopwords(tokenize(u_data))))\n",
    "    #print (u_data)\n",
    "    \n",
    "    docs = [article_data, u_data]\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    output = tfidf.fit_transform(docs)\n",
    "    cosine_similarities = linear_kernel(output[0:1], output).flatten()\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    output = tfidf.fit_transform(docs)\n",
    "    cosine_similarities = linear_kernel(output[0:1], output).flatten()\n",
    "    return cosine_similarities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trust(x, y, article_data):\n",
    "    x_seen = article_data[article_data[\"userHash\"] == int(x)]\n",
    "    x_seen_from_y = pd.DataFrame()\n",
    "    for i in range(0, len(x_seen)):\n",
    "        temp_sharer_list = ast.literal_eval(x_seen.iloc[i][\"sharerHash\"])\n",
    "        for s in temp_sharer_list:\n",
    "            if int(s) == int(y):\n",
    "                x_seen_from_y.append(x_seen.iloc[i])\n",
    "                break\n",
    "    if (len(x_seen_from_y) == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x_seen_from_y[x_seen_from_y[\"articleClicked\"] == True])/len(x_seen_from_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def engagementHistory(x, rand_article, article_data):\n",
    "    x_seen = article_data[article_data[\"userHash\"] == int(x)]\n",
    "    x_engaged = x_seen[x_seen[\"articleClicked\"] == True].reset_index()\n",
    "\n",
    "    timestamps = []\n",
    "    for index, row in valid_interactions.iterrows():\n",
    "        curr_time = ast.literal_eval(row[\"articleSeenTime\"])[len(ast.literal_eval(row[\"articleSeenTime\"]))-1]\n",
    "        timestamps.append(dt.strptime(curr_time, \"%a, %d %b %Y %H:%M:%S %Z\"))\n",
    "\n",
    "    timestamps_df = pd.DataFrame({'timestamps': timestamps})\n",
    "    x_engaged_t = x_engaged.join(timestamps_df)\n",
    "    x_engaged = x_engaged_t.sort_values([\"timestamps\"], ascending=False)\n",
    "    timedelta = getilogtimedelta(x, rand_article, article_data)\n",
    "    past_engagement_data = \"\"\n",
    "    for i in range(0, timedelta):\n",
    "        past_engagement_data = past_engagement_data + curr_article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_article_data = getArticleData(rand_article)\n",
    "\n",
    "    docs = [rand_article_data, past_engagement_data]\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    output = tfidf.fit_transform(docs)\n",
    "    cosine_similarities = linear_kernel(output[0:1], output).flatten()\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    output = tfidf.fit_transform(docs)\n",
    "\n",
    "    cosine_similarities = linear_kernel(output[0:1], output).flatten()\n",
    "    return cosine_similarities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getArticleData(rand_article):\n",
    "    comments = ast.literal_eval(rand_article[\"articleComments\"])[0]\n",
    "    title = rand_article[\"articleTitle\"]\n",
    "    if (comments == \"NULL\"):\n",
    "        comments = \"\"\n",
    "    if (title == \"NULL\"):\n",
    "        title = \"\"\n",
    "    article_data = str(comments) + \" \" + str(title)\n",
    "    return article_data\n",
    "\n",
    "def getUserData(userHash, users): #user_info is a df that is a single row\n",
    "    user_info = users[users[\"userHash\"] == userHash].reset_index()\n",
    "    u_data = \"\"\n",
    "    if str(user_info[\"currentLocation\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"currentLocation\"][0]) + \" \"\n",
    "    if str(user_info[\"education1location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"education1location\"][0]) + \" \"\n",
    "    if str(user_info[\"education1name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"education1name\"][0]) + \" \"\n",
    "    if str(user_info[\"education2location\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education2location\"][0]) + \" \"\n",
    "    if str(user_info[\"education2name\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education2name\"][0]) + \" \"\n",
    "    if str(user_info[\"education3location\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education3location\"][0]) + \" \"\n",
    "    if str(user_info[\"education3name\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education3name\"][0]) + \" \"\n",
    "    if str(user_info[\"gender\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"gender\"][0]) + \" \"\n",
    "    if str(user_info[\"hometown\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"hometown\"][0]) + \" \"\n",
    "    if str(user_info[\"language1\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language1\"][0]) + \" \"\n",
    "    if str(user_info[\"language2\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language2\"][0]) + \" \"\n",
    "    if str(user_info[\"language3\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language3\"][0]) + \" \"\n",
    "    if str(user_info[\"otherLocation1\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"otherLocation1\"][0]) + \" \"\n",
    "    if str(user_info[\"otherLocation2\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"otherLocation2\"][0]) + \" \"\n",
    "    if str(user_info[\"work1location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1location\"][0]) + \" \"\n",
    "    if str(user_info[\"work1name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1name\"][0]) + \" \"\n",
    "    if str(user_info[\"work1role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1role\"][0]) + \" \"\n",
    "    if str(user_info[\"work2location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2location\"][0]) + \" \"\n",
    "    if str(user_info[\"work2name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2name\"][0]) + \" \"\n",
    "    if str(user_info[\"work2role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2role\"][0]) + \" \"\n",
    "    if str(user_info[\"work3location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3location\"][0]) + \" \"\n",
    "    if str(user_info[\"work3name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3name\"][0]) + \" \"\n",
    "    if str(user_info[\"work3role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3role\"][0]) + \" \"\n",
    "    if str(user_info[\"work4location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4location\"][0]) + \" \"\n",
    "    if str(user_info[\"work4name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4name\"][0]) + \" \"\n",
    "    if str(user_info[\"work4role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4role\"][0]) + \" \"\n",
    "    return u_data\n",
    "\n",
    "def getUserArticleSim(article_data, u_data):\n",
    "    docs = [article_data, u_data]\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    output = tfidf.fit_transform(docs)\n",
    "    cosine_similarities = linear_kernel(output[0:1], output).flatten()\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_userHashes = users[\"userHash\"]\n",
    "articles_userHashes = forgraph[\"userHash\"]\n",
    "valid_userHashes = list(set(users_userHashes) & set(articles_userHashes))\n",
    "\n",
    "valid_interactions = forgraph[forgraph[\"userHash\"].isin(valid_userHashes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "while len(features) < 1000:\n",
    "    print(count)\n",
    "    random_userHash = random.choice(valid_userHashes)\n",
    "    user_articles = valid_interactions[valid_interactions['userHash'] == random_userHash]\n",
    "    engaged_articles = user_articles[user_articles[\"articleClicked\"] == True]\n",
    "    random_valid_article = engaged_articles.sample(n=1).reset_index().iloc(0)\n",
    "    random_valid_article = random_valid_article[0]\n",
    "    \n",
    "    currSharerHash = ast.literal_eval(random_valid_article[\"sharerHash\"])[0]\n",
    "    \n",
    "    timestamp = ast.literal_eval(random_valid_article[\"articleSeenTime\"])[0]\n",
    "    timestamp = dt.strptime(timestamp, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    beforetime_DF = pd.DataFrame()\n",
    "    for index, row in valid_interactions.iterrows():\n",
    "        for i in ast.literal_eval(row[\"articleSeenTime\"]):\n",
    "            date_obj = dt.strptime(i, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "            if date_obj <= timestamp:\n",
    "                beforetime_DF = beforetime_DF.append(row, ignore_index=True)\n",
    "                break\n",
    "    \n",
    "    not_engaged_articles = beforetime_DF[beforetime_DF[\"articleClicked\"] == False]\n",
    "    random_notengaged_article = not_engaged_articles.sample(n=1).reset_index().iloc(0)\n",
    "    random_notengaged_article = random_notengaged_article[0]\n",
    "    notengaged_sharerHash = ast.literal_eval(random_notengaged_article[\"sharerHash\"])[0]\n",
    "    \n",
    "    s_t = thematicInterest(random_userHash, users, random_valid_article)\n",
    "    s_f = thematicInterest(random_userHash, users, random_notengaged_article)\n",
    "    t_t = trust(random_userHash, currSharerHash, beforetime_DF)\n",
    "    t_f = trust(random_userHash, notengaged_sharerHash, beforetime_DF)\n",
    "    h_t = engagementHistory(random_userHash, random_valid_article, beforetime_DF)\n",
    "    h_f = engagementHistory(random_userHash, random_notengaged_article, beforetime_DF)\n",
    "    \n",
    "    features.append([s_t, t_t, h_t, 1])\n",
    "    features.append([s_f, t_f, h_f, 0])\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered1 = articles_2[articles_2['sharerHash'] != '[]']\n",
    "filtered2 = filtered1[filtered1['articleTitle'] != 'nan']\n",
    "filtered3 = filtered2[filtered2['articlePopularity'] != '[]']\n",
    "filtered4 = filtered3[filtered3['articleComments'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = filtered4[['userHash', 'articleTitle', 'sharerHash', 'articlePopularity', 'articleComments', 'articleSeenTime', 'articleClicked']].copy().reset_index()\n",
    "y = filtered4['articleClicked'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "userData = []\n",
    "comments = []\n",
    "user_sharer_pops = []\n",
    "times = []\n",
    "y = []\n",
    "p_count = 0\n",
    "n_count = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    for j in range(0, len(ast.literal_eval(X.iloc[i][\"sharerHash\"]))):\n",
    "        if X.iloc[i][\"articleClicked\"] == True and p_count < 5000:\n",
    "            p_count = p_count + 1\n",
    "            titles.append(str(X.iloc[i][\"articleTitle\"]))\n",
    "            currUserHash = int(X.iloc[i][\"userHash\"])\n",
    "            currUserData = getUserData(X.iloc[i][\"userHash\"], users)\n",
    "            userData.append(currUserData)\n",
    "            comments.append(ast.literal_eval(X.iloc[i][\"articleComments\"])[j])\n",
    "            times.append(ast.literal_eval(X.iloc[i][\"articleSeenTime\"][j]))\n",
    "            currSharerHash = int(ast.literal_eval(X.iloc[i][\"sharerHash\"])[j])\n",
    "            pops_temp = ast.literal_eval(X.iloc[i][\"articlePopularity\"])\n",
    "            curr_pop = pops_temp[j]\n",
    "            try:\n",
    "                curr_pop= int(curr_pop)\n",
    "            except:\n",
    "                if curr_pop.endswith('K'):\n",
    "                    num_len = len(curr_pop)\n",
    "                    num = curr_pop[:-1]\n",
    "                    curr_pop = float(num)*1000\n",
    "                else:\n",
    "                    curr_pop = 0\n",
    "            user_sharer_pops.append([currUserHash, currSharerHash, curr_pop])\n",
    "            y.append(1)\n",
    "        elif X.iloc[i][\"articleClicked\"] == False and n_count < 5000:\n",
    "            n_count = n_count + 1\n",
    "            titles.append(str(X.iloc[i][\"articleTitle\"]))\n",
    "            currUserHash = int(X.iloc[i][\"userHash\"])\n",
    "            currUserData = getUserData(X.iloc[i][\"userHash\"], users)\n",
    "            userData.append(currUserData)\n",
    "            comments.append(ast.literal_eval(X.iloc[i][\"articleComments\"])[j])\n",
    "            times.append(ast.literal_eval(X.iloc[i][\"articleSeenTime\"][j]))\n",
    "            currSharerHash = int(ast.literal_eval(X.iloc[i][\"sharerHash\"])[j])\n",
    "            pops_temp = ast.literal_eval(X.iloc[i][\"articlePopularity\"])\n",
    "            curr_pop = pops_temp[j]\n",
    "            try:\n",
    "                curr_pop= int(curr_pop)\n",
    "            except:\n",
    "                if curr_pop.endswith('K'):\n",
    "                    num_len = len(curr_pop)\n",
    "                    num = curr_pop[:-1]\n",
    "                    curr_pop = float(num)*1000\n",
    "                else:\n",
    "                    curr_pop = 0\n",
    "            user_sharer_pops.append([currUserHash, currSharerHash, curr_pop])\n",
    "            y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles_tfidf = TfidfVectorizer(stop_words='english')\n",
    "titles_output = titles_tfidf.fit_transform(titles)\n",
    "\n",
    "userdata_tfidf = TfidfVectorizer(stop_words='english')\n",
    "userdata_output = userdata_tfidf.fit_transform(userData)\n",
    "\n",
    "comments_tfidf = TfidfVectorizer(stop_words='english')\n",
    "comments_output = comments_tfidf.fit_transform(comments)\n",
    "\n",
    "times = [int(i) for dt.timestamp(i) in times]\n",
    "all_data_temp = hstack([titles_output, userdata_output, comments_output], format='csr')\n",
    "user_sharer_pops_sp = sparse.csr_matrix(np.asarray(user_sharer_pops))\n",
    "all_data = hstack([all_data_temp, user_sharer_pops_sp, times], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=7, max_features=739, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.fit(all_data, y, cross_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
