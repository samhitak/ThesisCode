{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "import math\n",
    "from random import randint\n",
    "import random\n",
    "import datetime\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv('users.csv', encoding = \"ISO-8859-1\")\n",
    "articles = pd.read_csv('articles.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeGraph(timestamp, graph_data):\n",
    "    timestamp = dt.strptime(timestamp, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    beforetime_DF = pd.DataFrame()\n",
    "    for i in range(0, len(graph_data)):\n",
    "        temp_date_list = ast.literal_eval(graph_data.iloc[[i]][\"articleSeenTime\"][i])\n",
    "        for k in range(0, len(temp_date_list)):\n",
    "            temp_date = temp_date_list[k]\n",
    "            date_obj = dt.strptime(temp_date, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "            if date_obj <= (timestamp + datetime.timedelta(minutes = 10)):\n",
    "                beforetime_DF = beforetime_DF.append(graph_data.iloc[[i]], ignore_index=True)\n",
    "                break\n",
    "                \n",
    "    sharerHash_list = beforetime_DF['sharerHash']\n",
    "    userHash_list = beforetime_DF[\"userHash\"]\n",
    "    \n",
    "    edges_list = []\n",
    "    for i in range(0, len(sharerHash_list)):\n",
    "        temp = ast.literal_eval(sharerHash_list[i])\n",
    "        for k in range(0, len(temp)):\n",
    "            edges_list.append((str(userHash_list[i]), temp[k]))\n",
    "            \n",
    "    weighted_edges = []\n",
    "    i = 0\n",
    "    while i < len(edges_list)-2:\n",
    "        weight = 1\n",
    "        while ((edges_list[i][0] == edges_list[i+1][0]) and (edges_list[i][1] == edges_list[i+1][1])):\n",
    "            weight = weight + 1\n",
    "            i = i + 1\n",
    "        weighted_edges.append((edges_list[i][0], edges_list[i][1], weight))\n",
    "        i = i + 1\n",
    "        \n",
    "    interactions = nx.DiGraph()\n",
    "    interactions.add_weighted_edges_from(weighted_edges)\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weightedCommonNeighbors(x, y, currGraph):\n",
    "    x_neighbors = currGraph.neighbors(x)\n",
    "    y_neighbors = currGraph.neighbors(y)\n",
    "    x_y_intersection = list(set(x_neighbors) & set(y_neighbors))\n",
    "    score = 0\n",
    "    for n in x_y_intersection:\n",
    "        x_n_weight = currGraph.edge[x][n]['weight']\n",
    "        y_n_weight = currGraph.edge[y][n]['weight']\n",
    "        score = score + (x_n_weight + y_n_weight)/2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weightedAdamicAdar(x, y, currGraph):\n",
    "    x_neighbors = currGraph.neighbors(x)\n",
    "    y_neighbors = currGraph.neighbors(y)\n",
    "    x_y_intersection = list(set(x_neighbors) & set(y_neighbors))\n",
    "    score = 0\n",
    "    for z in x_y_intersection:\n",
    "        x_z_weight = currGraph.edge[x][z]['weight']\n",
    "        y_z_weight = currGraph.edge[y][z]['weight']\n",
    "        z_neighbors = currGraph.neighbors(z)\n",
    "        z_score = 0\n",
    "        for z_prime in z_neighbors:\n",
    "            z_z_prime_weight = currGraph.edge[z][z_prime]['weight']\n",
    "            z_score = z_score + z_z_prime_weight\n",
    "        if z_score == 0:\n",
    "            score = score + ((x_z_weight + y_z_weight)/2)\n",
    "        else:\n",
    "            score = score + (((x_z_weight + y_z_weight)/2) * (1/math.log10(z_score)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weightedPrefAttachment(x, y, currGraph):\n",
    "    x_neighbors = currGraph.neighbors(x)\n",
    "    y_neighbors = currGraph.neighbors(y)\n",
    "    x_sum = 0.5\n",
    "    for x_prime in x_neighbors:\n",
    "        x_sum = x_sum + currGraph.edge[x][x_prime]['weight']\n",
    "    y_sum = 0.5\n",
    "    for y_prime in y_neighbors:\n",
    "        y_sum = y_sum + currGraph.edge[y][y_prime]['weight']\n",
    "    score = x_sum * y_sum\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get valid interactions set to randomly select from\n",
    "forgraph = articles[articles['sharerHash'] != '[]']\n",
    "forgraph = forgraph.reset_index()\n",
    "\n",
    "users_userHashes = users[\"userHash\"]\n",
    "users_userHashes = [int(i) for i in users_userHashes]\n",
    "articles_userHashes = forgraph[\"userHash\"]\n",
    "articles_userHashes = [int(i) for i in articles_userHashes]\n",
    "valid_userHashes = list(set(users_userHashes) & set(articles_userHashes))\n",
    "valid_interactions = forgraph[forgraph[\"userHash\"].isin(valid_userHashes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_counter = 0\n",
    "while len(test_results) < 1000:\n",
    "    print(test_counter)\n",
    "    # to check a positive\n",
    "    random_userHash = random.choice(valid_userHashes)\n",
    "    user_articles = valid_interactions[valid_interactions['userHash'] == random_userHash]\n",
    "    random_valid_article = user_articles.sample(n=1).reset_index().iloc(0)\n",
    "    random_valid_article = random_valid_article[0]\n",
    "    \n",
    "    \n",
    "    timestamp = ast.literal_eval(random_valid_article[\"articleSeenTime\"])[0]\n",
    "    v = valid_interactions.reset_index()\n",
    "    g = makeGraph(timestamp, v)\n",
    "    \n",
    "    # to check a negative\n",
    "    random_sharerHash_neg = random.choice(g.nodes())\n",
    "    while((str(random_userHash), str(random_sharerHash_neg)) in g.edges_iter()):\n",
    "        random_sharerHash_neg = random.choice(g.nodes())\n",
    "    \n",
    "    \n",
    "    currUserHash = str(random_userHash)\n",
    "    currSharerHash = str(ast.literal_eval(random_valid_article[\"sharerHash\"])[0])\n",
    "    currSharerHash_neg = str(random_sharerHash_neg)\n",
    "    # pos scores\n",
    "    n_pos = weightedCommonNeighbors(currUserHash, currSharerHash, g)\n",
    "    a_pos = weightedAdamicAdar(currUserHash, currSharerHash, g)\n",
    "    p_pos = weightedPrefAttachment(currUserHash, currSharerHash, g)\n",
    "    # neg scores\n",
    "    n_neg = weightedCommonNeighbors(currUserHash, currSharerHash_neg, g)\n",
    "    a_neg = weightedAdamicAdar(currUserHash, currSharerHash_neg, g)\n",
    "    p_neg = weightedPrefAttachment(currUserHash, currSharerHash_neg, g)\n",
    "\n",
    "    # get max scores of each type for this timestep in order to normalize\n",
    "    max_n = 0\n",
    "    max_a = 0\n",
    "    max_p = 0\n",
    "    for i in g.edges_iter():\n",
    "        curr_n = weightedCommonNeighbors(i[0], i[1], g)\n",
    "        if curr_n > max_n: \n",
    "            max_n = curr_n\n",
    "        curr_a = weightedAdamicAdar(i[0], i[1], g)\n",
    "        if curr_a > max_a:\n",
    "            max_a = curr_a\n",
    "        curr_p = weightedPrefAttachment(i[0], i[1], g)\n",
    "        if curr_p > max_p:\n",
    "            max_p = curr_p\n",
    "    norm_n_pos = n_pos/max_n\n",
    "    norm_n_neg = n_neg/max_n\n",
    "    norm_a_pos = a_pos/max_a\n",
    "    norm_a_neg = a_neg/max_a\n",
    "    norm_p_pos = p_pos*10/max_p\n",
    "    norm_p_neg = p_neg/max_p\n",
    "\n",
    "    test_results.append((currUserHash, currSharerHash, norm_n_pos, norm_a_pos, norm_p_pos, 1))\n",
    "    test_results.append((currUserHash, currSharerHash_neg, norm_n_neg, norm_a_neg, norm_p_neg, 0))\n",
    "    test_counter = test_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_n = [i[2] for i in test_results]\n",
    "pred_a = [i[3] for i in test_results]\n",
    "pred_p = [i[4] for i in test_results]\n",
    "truth = [i[5] for i in test_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_n, tpr_n, thresholds_n = roc_curve(truth, pred_n)\n",
    "fpr_a, tpr_a, thesholds_a = roc_curve(truth, pred_a)\n",
    "fpr_p, tpr_p, thresholds_p = roc_curve(truth, pred_p)\n",
    "roc_auc_n = auc(fpr_n, tpr_n)\n",
    "roc_auc_a = auc(fpr_a, tpr_a)\n",
    "roc_auc_p = auc(fpr_p, tpr_p)\n",
    "\n",
    "print(\"Area under the CN ROC curve : %f\" % roc_auc_n)\n",
    "print(\"Area under the AA ROC curve : %f\" % roc_auc_a)\n",
    "print (\"Area under the PA ROC curve : %f\" % roc_auc_p)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "plt.plot(fpr_n, tpr_n, color='green', label='Common Neighbors ROC curve (area = %0.2f)' % roc_auc_n)\n",
    "plt.plot(fpr_a, tpr_a, color='red', label='Adamic Adar ROC curve (area = %0.2f)' % roc_auc_a)\n",
    "plt.plot(fpr_p, tpr_p, color='blue', label='Preferential Attachment ROC curve (area = %0.2f)' % roc_auc_p)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Link Prediction ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('diffusion_basemethod_roc_1000.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
