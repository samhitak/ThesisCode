{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from scipy import sparse\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "import math\n",
    "from random import randint\n",
    "import random\n",
    "import datetime\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv('users.csv', encoding = \"ISO-8859-1\")\n",
    "articles = pd.read_csv('articles.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take interactions that have userHashes that are in the users table\n",
    "users_list = set(users[\"userHash\"])\n",
    "articles_2 = articles[articles[\"userHash\"].isin(users_list)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getUserData(userHash, users): #user_info is a df that is a single row\n",
    "    user_info = users[users[\"userHash\"] == userHash].reset_index()\n",
    "    u_data = \"\"\n",
    "    if str(user_info[\"currentLocation\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"currentLocation\"][0]) + \" \"\n",
    "    if str(user_info[\"education1location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"education1location\"][0]) + \" \"\n",
    "    if str(user_info[\"education1name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"education1name\"][0]) + \" \"\n",
    "    if str(user_info[\"education2location\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education2location\"][0]) + \" \"\n",
    "    if str(user_info[\"education2name\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education2name\"][0]) + \" \"\n",
    "    if str(user_info[\"education3location\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education3location\"][0]) + \" \"\n",
    "    if str(user_info[\"education3name\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"education3name\"][0]) + \" \"\n",
    "    if str(user_info[\"gender\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"gender\"][0]) + \" \"\n",
    "    if str(user_info[\"hometown\"][0]) != \"NaN\":\n",
    "        u_data = u_data + str(user_info[\"hometown\"][0]) + \" \"\n",
    "    if str(user_info[\"language1\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language1\"][0]) + \" \"\n",
    "    if str(user_info[\"language2\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language2\"][0]) + \" \"\n",
    "    if str(user_info[\"language3\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"language3\"][0]) + \" \"\n",
    "    if str(user_info[\"otherLocation1\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"otherLocation1\"][0]) + \" \"\n",
    "    if str(user_info[\"otherLocation2\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"otherLocation2\"][0]) + \" \"\n",
    "    if str(user_info[\"work1location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1location\"][0]) + \" \"\n",
    "    if str(user_info[\"work1name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1name\"][0]) + \" \"\n",
    "    if str(user_info[\"work1role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work1role\"][0]) + \" \"\n",
    "    if str(user_info[\"work2location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2location\"][0]) + \" \"\n",
    "    if str(user_info[\"work2name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2name\"][0]) + \" \"\n",
    "    if str(user_info[\"work2role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work2role\"][0]) + \" \"\n",
    "    if str(user_info[\"work3location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3location\"][0]) + \" \"\n",
    "    if str(user_info[\"work3name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3name\"][0]) + \" \"\n",
    "    if str(user_info[\"work3role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work3role\"][0]) + \" \"\n",
    "    if str(user_info[\"work4location\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4location\"][0]) + \" \"\n",
    "    if str(user_info[\"work4name\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4name\"][0]) + \" \"\n",
    "    if str(user_info[\"work4role\"][0]) != \"nan\":\n",
    "        u_data = u_data + str(user_info[\"work4role\"][0]) + \" \"\n",
    "    return u_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered1 = articles_2[articles_2['sharerHash'] != '[]']\n",
    "filtered2 = filtered1[filtered1['articleTitle'] != 'nan']\n",
    "filtered3 = filtered2[filtered2['articlePopularity'] != '[]']\n",
    "filtered4 = filtered3[filtered3['articleComments'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = filtered4[['userHash', 'articleTitle', 'sharerHash', 'articlePopularity', 'articleComments', 'articleClickedTime', 'articleClicked']].copy().reset_index()\n",
    "y = filtered4['articleClicked'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "userData = []\n",
    "comments = []\n",
    "user_sharer_pops = []\n",
    "times = []\n",
    "y = []\n",
    "p_count = 0\n",
    "n_count = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    for j in range(0, len(ast.literal_eval(X.iloc[i][\"sharerHash\"]))):\n",
    "        if X.iloc[i][\"articleClicked\"] == True and p_count < 5000:\n",
    "            p_count = p_count + 1\n",
    "            titles.append(str(X.iloc[i][\"articleTitle\"]))\n",
    "            currUserHash = int(X.iloc[i][\"userHash\"])\n",
    "            currUserData = getUserData(X.iloc[i][\"userHash\"], users)\n",
    "            userData.append(currUserData)\n",
    "            comments.append(ast.literal_eval(X.iloc[i][\"articleComments\"])[j])\n",
    "            currSharerHash = int(ast.literal_eval(X.iloc[i][\"sharerHash\"])[j])\n",
    "            pops_temp = ast.literal_eval(X.iloc[i][\"articlePopularity\"])\n",
    "            curr_pop = pops_temp[j]\n",
    "            try:\n",
    "                curr_pop= int(curr_pop)\n",
    "            except:\n",
    "                if curr_pop.endswith('K'):\n",
    "                    num_len = len(curr_pop)\n",
    "                    num = curr_pop[:-1]\n",
    "                    curr_pop = float(num)*1000\n",
    "                else:\n",
    "                    curr_pop = 0\n",
    "            user_sharer_pops.append([currUserHash, currSharerHash, curr_pop])\n",
    "            y.append(1)\n",
    "        elif X.iloc[i][\"articleClicked\"] == False and n_count < 5000:\n",
    "            n_count = n_count + 1\n",
    "            titles.append(str(X.iloc[i][\"articleTitle\"]))\n",
    "            currUserHash = int(X.iloc[i][\"userHash\"])\n",
    "            currUserData = getUserData(X.iloc[i][\"userHash\"], users)\n",
    "            userData.append(currUserData)\n",
    "            comments.append(ast.literal_eval(X.iloc[i][\"articleComments\"])[j])\n",
    "            currSharerHash = int(ast.literal_eval(X.iloc[i][\"sharerHash\"])[j])\n",
    "            pops_temp = ast.literal_eval(X.iloc[i][\"articlePopularity\"])\n",
    "            curr_pop = pops_temp[j]\n",
    "            try:\n",
    "                curr_pop= int(curr_pop)\n",
    "            except:\n",
    "                if curr_pop.endswith('K'):\n",
    "                    num_len = len(curr_pop)\n",
    "                    num = curr_pop[:-1]\n",
    "                    curr_pop = float(num)*1000\n",
    "                else:\n",
    "                    curr_pop = 0\n",
    "            user_sharer_pops.append([currUserHash, currSharerHash, curr_pop])\n",
    "            y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles_tfidf = TfidfVectorizer(stop_words='english')\n",
    "titles_output = titles_tfidf.fit_transform(titles)\n",
    "\n",
    "userdata_tfidf = TfidfVectorizer(stop_words='english')\n",
    "userdata_output = userdata_tfidf.fit_transform(userData)\n",
    "\n",
    "comments_tfidf = TfidfVectorizer(stop_words='english')\n",
    "comments_output = comments_tfidf.fit_transform(comments)\n",
    "\n",
    "all_data_temp = hstack([titles_output, userdata_output, comments_output], format='csr')\n",
    "user_sharer_pops_sp = sparse.csr_matrix(np.asarray(user_sharer_pops))\n",
    "all_data = hstack([all_data_temp, user_sharer_pops_sp], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=7, max_features=739, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.fit(all_data, y, cross_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
